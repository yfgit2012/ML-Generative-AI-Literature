# Generative AI Literature

#### Large Language Model
* LLaMA: Open and Efficient Foundation Language Models (Feb 2023) &nbsp;[Link](https://arxiv.org/abs/2302.13971) &nbsp;[PDF](https://arxiv.org/pdf/2302.13971.pdf)
* Stanford Alpaca: An Instruction-following LLaMA Model (March 2023) &nbsp;[Link](https://crfm.stanford.edu/2023/03/13/alpaca.html)[Github](https://github.com/tatsu-lab/stanford_alpaca)
* Llama 2: Open Foundation and Fine-Tuned Chat Models (Jul 2023) &nbsp;[Link](https://arxiv.org/abs/2307.09288) &nbsp;[PDF](https://arxiv.org/pdf/2307.09288.pdf)
* The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only (Jun 2023) &nbsp;[Link](https://arxiv.org/abs/2306.01116) &nbsp;[PDF](https://arxiv.org/pdf/2306.01116.pdf)
* Mistral 7B (Oct 2023) &nbsp;[Link](https://arxiv.org/abs/2310.06825) &nbsp;[PDF](https://arxiv.org/pdf/2310.06825.pdf)

#### RAG
* Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Apr 2021) &nbsp;[Link](https://arxiv.org/abs/2005.11401) &nbsp;[PDF](https://arxiv.org/pdf/2005.11401.pdf)
* Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering (Jun 2021) &nbsp;[Link](https://arxiv.org/abs/2106.11517) &nbsp;[PDF](https://arxiv.org/pdf/2106.11517.pdf)
* Benchmarking Large Language Models in Retrieval-Augmented Generation (Sep 2023) &nbsp;[Link](https://arxiv.org/abs/2309.01431) &nbsp;[PDF](https://arxiv.org/pdf/2309.01431.pdf)      

#### Fine Tuning
* Instruction Tuning for Large Language Models: A Survey (Oct 2023) &nbsp;[Link](https://arxiv.org/abs/2308.10792) &nbsp;[PDF](https://arxiv.org/pdf/2308.10792.pdf) 
* Self-Instruct: Aligning Language Models with Self-Generated Instructions (May 2023) &nbsp;[Link](https://arxiv.org/abs/2212.10560) &nbsp;[PDF](https://arxiv.org/pdf/2212.10560.pdf)
* On the Exploitability of Instruction Tuning (Jun 2023) &nbsp;[Link](https://arxiv.org/abs/2306.17194) &nbsp;[PDF](https://arxiv.org/pdf/2306.17194.pdf)
* LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (Jun 2023) &nbsp;[Link](https://arxiv.org/abs/2303.16199) &nbsp;[PDF](https://arxiv.org/pdf/2303.16199.pdf)

#### Parameter Efficient Fine Tuning
* Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning (Aug 2022) &nbsp;[Link](https://arxiv.org/abs/2205.05638?ref=txt.cohere.com) &nbsp;[PDF](https://arxiv.org/pdf/2205.05638.pdf)  
* The Power of Scale for Parameter-Efficient Prompt Tuning (Sep 2021) &nbsp;[Link](https://arxiv.org/abs/2104.08691) &nbsp;[PDF](https://arxiv.org/pdf/2104.08691.pdf)
* LoRA: Low-Rank Adaptation of Large Language Models (Oct 2021) &nbsp;[Link](https://arxiv.org/abs/2106.09685) &nbsp;[PDF](https://arxiv.org/pdf/2106.09685.pdf)
* Parameter-Efficient Fine-Tuning without Introducing New Latency (May 2023) &nbsp;[Link](https://arxiv.org/abs/2305.16742) &nbsp;[PDF](https://arxiv.org/pdf/2305.16742.pdf)

#### Reinforcement Learning 
* Training language models to follow instructions with human feedback (Mar 2022) &nbsp;[Link](https://arxiv.org/abs/2203.02155) &nbsp;[PDF](https://arxiv.org/pdf/2203.02155.pdf)
* Secrets of RLHF in Large Language Models Part I: PPO (Jul 2023) &nbsp;[Link](https://arxiv.org/abs/2307.04964) &nbsp;[PDF](https://arxiv.org/pdf/2307.04964.pdf)
* RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback (Sep 2023) &nbsp;[Link](https://arxiv.org/abs/2309.00267) &nbsp;[PDF](https://arxiv.org/pdf/2309.00267.pdf)
* Constitutional AI: Harmlessness from AI Feedback (Dec 2022) &nbsp;[Link](https://arxiv.org/abs/2212.08073) &nbsp;[PDF](https://arxiv.org/pdf/2212.08073.pdf)

  
