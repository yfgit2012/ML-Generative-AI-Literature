# Generative AI Literature

#### Large Language Model
* LLaMA: Open and Efficient Foundation Language Models (Feb 2023) &nbsp;[Link](https://arxiv.org/abs/2302.13971) &nbsp;[PDF](https://arxiv.org/pdf/2302.13971.pdf)
* Llama 2: Open Foundation and Fine-Tuned Chat Models (Jul 2023) &nbsp;[Link](https://arxiv.org/abs/2307.09288) &nbsp;[PDF](https://arxiv.org/pdf/2307.09288.pdf)
* The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only (Jun 2023) &nbsp;[Link](https://arxiv.org/abs/2306.01116) &nbsp;[PDF](https://arxiv.org/pdf/2306.01116.pdf)
* Mistral 7B (Oct 2023) &nbsp;[Link](https://arxiv.org/abs/2310.06825) &nbsp;[PDF](https://arxiv.org/pdf/2310.06825.pdf)

#### RAG
* Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Apr 2021) &nbsp;[Link](https://arxiv.org/abs/2005.11401) &nbsp;[PDF](https://arxiv.org/pdf/2005.11401.pdf)
* Benchmarking Large Language Models in Retrieval-Augmented Generation (Sep 2023) &nbsp;[Link](https://arxiv.org/abs/2309.01431) &nbsp;[PDF](https://arxiv.org/pdf/2309.01431.pdf)      
* Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering (Jun 2021) &nbsp;[Link](https://arxiv.org/abs/2106.11517) &nbsp;[PDF](https://arxiv.org/pdf/2106.11517.pdf)

#### Parameter Efficient Fine Tuning
* Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning (Aug 2022) &nbsp;[Link](https://arxiv.org/abs/2205.05638?ref=txt.cohere.com) &nbsp;[PDF](https://arxiv.org/pdf/2205.05638.pdf)
* Parameter-Efficient Fine-Tuning without Introducing New Latency (May 2023) &nbsp;[Link](https://arxiv.org/abs/2305.16742) &nbsp;[PDF](https://arxiv.org/pdf/2305.16742.pdf)
* The Power of Scale for Parameter-Efficient Prompt Tuning (Sep 2021) &nbsp;[Link](https://arxiv.org/abs/2104.08691) &nbsp;[PDF](https://arxiv.org/pdf/2104.08691.pdf)
* LoRA: Low-Rank Adaptation of Large Language Models (Oct 2021) &nbsp;[Link](https://arxiv.org/abs/2106.09685) &nbsp;[PDF](https://arxiv.org/pdf/2106.09685.pdf)

#### Reinforcement Learning 
*

#### Self-Instruct  
* 
